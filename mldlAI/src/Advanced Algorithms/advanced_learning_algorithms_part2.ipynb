{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fbd8dc",
   "metadata": {},
   "source": [
    "### Neural Network Training: Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24eeaca",
   "metadata": {},
   "source": [
    "Given a training set of (x,y) examples, how do you train the NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=25, activation=\"sigmoid\"),\n",
    "    Dense(units=15, activation=\"sigmoid\"),\n",
    "    Dense(units=1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "model.compile(loss=BinaryFocalCrossentropy)#Specify the loss function\n",
    "model.fit(X,Y,epochs=100)#Train for 100 times, e.g. number of steps in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4ec9e",
   "metadata": {},
   "source": [
    "Therefore:\n",
    "1. Specify the model that tells tensorflow to compute for the inference (e.g. Sequential)\n",
    "2. Compile the model using a specific loss function\n",
    "3. Train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60152eca",
   "metadata": {},
   "source": [
    "### Training Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c937ab3",
   "metadata": {},
   "source": [
    "Model training steps:\n",
    "1. Specify how to compute output given input $x$ and parameters $w$ and $b$ (define the model)\n",
    "2. Specify the loss (for logistic regression: $f_{\\vec{w},b}(\\vec{x}^i,y^i) = -(y^i)log(f_{\\vec{w},b}(\\vec{x}^i)) - (1-y^i)log(1-f_{\\vec{w},b}(\\vec{x}^i))$)\n",
    "3. and the cost function (for logistic regression: $J(\\vec{w},b) = -\\frac{1}{m}\\sum_{i=1}^{m}[(y^i)log(f_{\\vec{w},b}(\\vec{x}^i)) + (1-y^i)log(1-f_{\\vec{w},b}(\\vec{x}^i))]$)\n",
    "4. Train the data to minimize the cost function: **Using gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce85170",
   "metadata": {},
   "source": [
    "**Remember**: The *loss function* is for a single example and the *cost function* is the loss over the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b209f",
   "metadata": {},
   "source": [
    "Now, how these steps map to training a NN:\n",
    "1. *model = Sequential([ Dense(...), Dense(...),...])*\n",
    "    * Specifies the entire architecture of the NN layer by layer\n",
    "2. *model.compile(loss=BinaryCrossentropy())* specifying the loss, taking this average over the training set gives you the cost!\n",
    "    * *BinaryCrossentropy* is referring to the exact loss function described above that is used for logistic regression\n",
    "    * For regression you might use *MeanSquaredError()* loss \n",
    "3. *model.fit(X,y, epochs=100)*\n",
    "    * Within the *fit* it computes the derivatives for gradient descent using **backpropagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc3500",
   "metadata": {},
   "source": [
    "### Alternatives to the Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971de50",
   "metadata": {},
   "source": [
    "Recall the demand prediction for t-shirts. Using a sigmoid function assumes that people are either aware or not. **The degree to which buyers are e.g. aware may not be binary, there are degrees of awareness**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a93231",
   "metadata": {},
   "source": [
    "To have the activation value take on many values we can swap in a different function:  \n",
    "$$g(z) = max(0,z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb9852",
   "metadata": {},
   "source": [
    "This basically says that if $z<0$ then $g(z) = 0$ and if $z>0$ then $g(z) = z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420d836",
   "metadata": {},
   "source": [
    "This function is called **ReLU** and it stands for **Rectified Linear Unit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ddf9c",
   "metadata": {},
   "source": [
    "The **Linear activation function** is also another popular one and this is:\n",
    "$$g(z)=z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3821b5",
   "metadata": {},
   "source": [
    "### Choosing the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d2dd4",
   "metadata": {},
   "source": [
    "It turns out depending on the target label, there will be a fairly straight forward choice for these activation functions.\n",
    "\n",
    "**Output Layer**\n",
    "* Considering if you are working on a classification problem, then the *output layer* will surely be the *sigmoid* activation function as the output should be 1 or 0\n",
    "* If you are trying to predict stock prices, then the *output layer* would most likely be the *Linear* activation function, because then the output can be positive or negative\n",
    "* However, if you are trying to predict the pricing of a house then surely the *output layer* should be *ReLU* activation function\n",
    "\n",
    "**Hidden Layers**\n",
    "* *ReLU* is by far the most common choice in how NNs are trained\n",
    "* *Sigmoid* function is hardly ever used with the one exception: Use a *sigmoid* at the *output layer* when it is a Binary classification problem\n",
    "* This is because *ReLU* is computed faster. Also when calculating gradient descent it is faster since *sigmoid* is flat at two places and *ReLU* only at 1\n",
    "* *ReLU* also learns therefore faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54f776",
   "metadata": {},
   "source": [
    "### Why do we need activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18196cf5",
   "metadata": {},
   "source": [
    "What if we only used Linear activation functions? Then the model would just become a big *Linear regression* model and would not be able to fit to anything more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d7085",
   "metadata": {},
   "source": [
    "Alternatively, if we had to use the *Linear activation function* for the hidden layers and a *sigmoid activation function* for the output layer, it can be shown that the model becomes equivalent to **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c6996",
   "metadata": {},
   "source": [
    "Therefore, the common rule is do not use the *Linear activation function* for hidden layers, rather use *ReLU*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e94d0",
   "metadata": {},
   "source": [
    "### ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ef0b3",
   "metadata": {},
   "source": [
    "* This function is composed of different linear pieces (piecewise linear). \n",
    "* The slope remains consistent during the linear portion and then changes abruptly at transition points. \n",
    "* At every transition point a new linear function is added.\n",
    "* The *ReLU* can 'deactivate' portions to fit to the needed slope and then turn them on as each portion starts matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd95146",
   "metadata": {},
   "source": [
    "### Multiclas Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2470564",
   "metadata": {},
   "source": [
    "More than just two output labels.  \n",
    "\n",
    "For example instead of trying to recognize only 1 or 0, if you want to recognize entire zipcodes (many number), it is still classification as there is a limited number of values y can have.\n",
    "\n",
    "In previous examples the classification problems had \"groups\" of x's and o's, but now imagine there are several \"groups\" that all can be represented with different shapes like rectangles, triangles etc.\n",
    "\n",
    "Now you want to predict the chance that y=1(o's), y=2(x's), y=3(rectangles), or y=4(triangles).\n",
    "\n",
    "**Now you need a function that can have a decision boundary that splits these into four different categories and this is called *softmax***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99037e8a",
   "metadata": {},
   "source": [
    "### Softmax Regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a21ed4",
   "metadata": {},
   "source": [
    "*It is a generalization of logistic regression, which is a binary classification algorithm to the multiclass classification context.*  \n",
    "\n",
    "If we take the normal logistic regression. where $a_1 = g(z)$ then we have that $a_1$ is some number between 0 and 1. If it is trying to calculate the probability of the number being a 1 and the output is 0.71, then you have a 71% chance that it is 1.  \n",
    "\n",
    "But also, it is actually calculating the probability of it being 0 also, and that would then be the remainder. **Therefore the logistic regression calculates two values: Probability of it being a 1 (79%), and the probability of it being a 0 (21%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305638d",
   "metadata": {},
   "source": [
    "This works for 2 possible output values. Now lets look at Softmax regression where there are 4 possible output values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b042d51",
   "metadata": {},
   "source": [
    "It calculates:  \n",
    "$z_1 = \\vec{w}_1\\cdot\\vec{x} + b_1$  for y = 1  \n",
    "$z_2 = \\vec{w}_2\\cdot\\vec{x} + b_2$  for y = 2  \n",
    "$z_3 = \\vec{w}_3\\cdot\\vec{x} + b_3$  for y = 3  \n",
    "$z_4 = \\vec{w}_4\\cdot\\vec{x} + b_4$  for y = 4  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071ae47",
   "metadata": {},
   "source": [
    "Then to estimate the $a_1$ it does the following:\n",
    "$$a_1 = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e41d0",
   "metadata": {},
   "source": [
    "It effectively calculates $a_1$ estimate as the chance of $P(y=1)$ given the input features $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635d535",
   "metadata": {},
   "source": [
    "Therefore:  \n",
    "$$a_2 = \\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efeb0b",
   "metadata": {},
   "source": [
    "$$a_3 = \\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289cfb6",
   "metadata": {},
   "source": [
    "$$a_4 = \\frac{e^{z_4}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28740d17",
   "metadata": {},
   "source": [
    "Given the output values:  \n",
    "$a_1 = 0.30$  \n",
    "$a_2 = 0.20$  \n",
    "$a_3 = 0.15$  \n",
    "\n",
    "Then $a_4$ must be equal to $(1-0.3-0.2-0.15)=0.35$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a796951",
   "metadata": {},
   "source": [
    "In the general case you have that $(y = 1,2,3,...N)$:\n",
    "$$z_j = \\vec{w}_j\\cdot\\vec{x} + b_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ad15b",
   "metadata": {},
   "source": [
    "$$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^Ne^{z_k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c18c7",
   "metadata": {},
   "source": [
    "The loss function for Softmax regression is the following:\n",
    "\n",
    "$loss(a_1, a_2,...a_N,y)$ = $-loga_1$ if $y=1$, $-loga_2$ if $y=2$, $-loga_3$ if $y=3$, ....,  $-loga_N$ if $y=N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fbc86",
   "metadata": {},
   "source": [
    "Here we use the *Crossentropy* loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bf3f3",
   "metadata": {},
   "source": [
    "### NN with Softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842606b6",
   "metadata": {},
   "source": [
    "In order to build a NN to do *multiclass classification* we need to take the *Softmax* model and put it on the output layer of the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12733afb",
   "metadata": {},
   "source": [
    "E.g., if you have to classify 10 classes, then the output layer will be a *Softmax* activation layer with 10 outputs. Now it can give you the probability of any of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246e2f6",
   "metadata": {},
   "source": [
    "Different from the sigmoid or ReLU, $a_1$ is not only a function of $z_1$ (e.g. $a_1 = g(z_1)$) but now it is a function of all the '$z$' values, as seen from the formula:  \n",
    "$$a_1 = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbb2e8",
   "metadata": {},
   "source": [
    "**Tensorflow implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy())\n",
    "\n",
    "model.fit(X,Y,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1011e8d",
   "metadata": {},
   "source": [
    "### Improved Implementation of Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa94376",
   "metadata": {},
   "source": [
    "Depending on how you calculate the losses, the computer will have different round off errors, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37750e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000200000000000000\n"
     ]
    }
   ],
   "source": [
    "x1 = 2/10000\n",
    "print(f\"{x1:.18f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f41ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000199999999999978\n"
     ]
    }
   ],
   "source": [
    "x2 = 1 + (1/10000)-(1-1/10000)\n",
    "print(f\"{x2:.18f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622c0e2",
   "metadata": {},
   "source": [
    "There is a better way to computing losses that has less rounding errors and therefore more accurate predictions.  \n",
    "\n",
    "Having intermediate values has more rounding errors. E.g., in logistic regression if we calculate $a$ first separately and thereafter plug it into the loss function there will be rounding errors. **INSTEAD** it is better to plug the formula for $a$ directly into the loss function and calculate it straight.  \n",
    "\n",
    "To do this in code looks like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ae74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=10, activation='linear') #Previously it was 'logistic'\n",
    "])\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy(from_logits=True)) #This true argument makes it get computed more accurately\n",
    "\n",
    "model.fit(X,Y,epochs=100)\n",
    "\n",
    "#Take teh output value and map it to the logistic function in order to get the probability\n",
    "\n",
    "logit = model(X)\n",
    "f_x = tf.nn.sigmoid(logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84363d7",
   "metadata": {},
   "source": [
    "Now let's apply this same logic to the *Softmax* regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b897f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=10, activation='linear') #Previously it was 'softmax'\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True)) #Set again to true\n",
    "\n",
    "model.fit(X,Y,epochs=100)\n",
    "\n",
    "#Now because the model ouputs z1,...,z10 instead of a1,...,a10\n",
    "logits = model(X)\n",
    "f_x = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db12a65",
   "metadata": {},
   "source": [
    "### Classification with multiple outputs (Multi-label classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1200f42",
   "metadata": {},
   "source": [
    "Which is where associated with each image there could be multiple labels.  \n",
    "\n",
    "e.g., given a self-driving example, there is a single picture and it the picture there can be pedestrians, busses and/or cars. Associated with a single input (image) there can be 3 (or more!) labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6634a",
   "metadata": {},
   "source": [
    "This also means that the target output $y$ will be a vector of many numbers (3 in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e804ac",
   "metadata": {},
   "source": [
    "**How to build a NN for this?**\n",
    "* Possibly treat it as 3 different problems and build a NN for each 'label' e.g. first NN detects cars, second detects busses and third detects the pedestrians\n",
    "* Another way is to have a NN with an output layer of a vector that contains 3 numbers\n",
    "    * Each node can have a *sigmoid* activation function to predict the probability of each label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9fe1d",
   "metadata": {},
   "source": [
    "### Softmax Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32d5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def my_softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c3b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fca7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8978  \n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 0.3992\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1652  \n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 0.0922\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 0.0679\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0562\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.0498\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 0.0450\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.0417\n",
      "Epoch 10/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 0.0388\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step\n",
      "[[1.1515623e-03 6.3586971e-03 9.6904671e-01 2.3442948e-02]\n",
      " [9.9654901e-01 3.4358155e-03 1.5098634e-05 1.0084555e-08]]\n",
      "largest value 0.9999995 smallest value 7.411034e-14\n"
     ]
    }
   ],
   "source": [
    "#Obvious organization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(),\n",
    "              optimizer = tf.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "p_nonpreferred = model.predict(X_train)\n",
    "print(p_nonpreferred[:2])\n",
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5611043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 1.0921 \n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.4360\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.1970\n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.1184\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.0874\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 0.0721\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 0.0632\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.0574\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 0.0536\n",
      "Epoch 10/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.0505\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step\n",
      "two example output vectors:\n",
      " [[-2.8254275  -2.999236    3.3067744   0.20466296]\n",
      " [ 5.188727    0.53661877 -1.8023942  -3.0403824 ]]\n",
      "largest value:  10.703296 smallest value:  -16.560171\n"
     ]
    }
   ],
   "source": [
    "#Now the preferred way of doing it\n",
    "\n",
    "preferred_model = Sequential([\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(15, activation='relu'),\n",
    "    Dense(4, activation='linear')\n",
    "])\n",
    "\n",
    "preferred_model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(f\"two example output vectors:\\n {p_preferred[:2]}\")\n",
    "print(\"largest value: \",np.max(p_preferred), \"smallest value: \", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b517e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output vectors:\n",
      " [[2.0704423e-03 1.7401201e-03 9.5333314e-01 4.2856321e-02]\n",
      " [9.8938560e-01 9.4401883e-03 9.1024896e-04 2.6394229e-04]]\n",
      "largest value:  0.9999908 smallest value 7.082175e-11\n"
     ]
    }
   ],
   "source": [
    "# The output predictions are not yet probabilities!\n",
    "\n",
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
    "print(\"largest value: \",np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d3f091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.8254275  -2.999236    3.3067744   0.20466296], category: 2\n",
      "[ 5.188727    0.53661877 -1.8023942  -3.0403824 ], category: 0\n",
      "[ 3.8686466  0.6968279 -1.4726437 -2.415347 ], category: 0\n",
      "[-1.4387716   4.5538263  -3.151655   -0.32492137], category: 1\n",
      "[-0.11418828 -4.5038643   5.19038    -1.9442217 ], category: 2\n"
     ]
    }
   ],
   "source": [
    "#To find the most likely category, the softmax is not required. You can find the index of the largest output using np.argmax()\n",
    "for i in range(5):\n",
    "    print(f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf83c6",
   "metadata": {},
   "source": [
    "### Multi-class Classification Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96d8f6",
   "metadata": {},
   "source": [
    "Imagine an example of taking in a photo and having to classify subjects in the photo as {dog, cat, horse, other}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses Scikit learn to create a training set with 4 categories\n",
    "classes = 4\n",
    "m = 100\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "std = 1.0\n",
    "X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd30c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique classes [0 1 2 3]\n",
      "class representation [3 3 3 0 3 3 3 3 2 0]\n",
      "shape of the X_train: (100, 2), shape of the y_train: (100,)\n"
     ]
    }
   ],
   "source": [
    "#show unique classes in data set\n",
    "print(f\"unique classes {np.unique(y_train)}\")\n",
    "#show how classes are represented\n",
    "print(f\"class representation {y_train[:10]}\")\n",
    "#show shapes of the dataset\n",
    "print(f\"shape of the X_train: {X_train.shape}, shape of the y_train: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f621cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7661  \n",
      "Epoch 2/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6083 \n",
      "Epoch 3/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4886 \n",
      "Epoch 4/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3901 \n",
      "Epoch 5/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3091 \n",
      "Epoch 6/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2421 \n",
      "Epoch 7/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1872 \n",
      "Epoch 8/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1424 \n",
      "Epoch 9/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1055 \n",
      "Epoch 10/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0748 \n",
      "Epoch 11/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0488 \n",
      "Epoch 12/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.0260\n",
      "Epoch 13/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0055 \n",
      "Epoch 14/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9865 \n",
      "Epoch 15/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9683 \n",
      "Epoch 16/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9505 \n",
      "Epoch 17/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9325 \n",
      "Epoch 18/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9140 \n",
      "Epoch 19/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8947 \n",
      "Epoch 20/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8742 \n",
      "Epoch 21/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8521 \n",
      "Epoch 22/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8281 \n",
      "Epoch 23/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8021 \n",
      "Epoch 24/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7740 \n",
      "Epoch 25/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7438 \n",
      "Epoch 26/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7123 \n",
      "Epoch 27/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6800 \n",
      "Epoch 28/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6476 \n",
      "Epoch 29/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6157 \n",
      "Epoch 30/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5850 \n",
      "Epoch 31/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5559 \n",
      "Epoch 32/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5285 \n",
      "Epoch 33/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5030 \n",
      "Epoch 34/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4792 \n",
      "Epoch 35/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4571 \n",
      "Epoch 36/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4365 \n",
      "Epoch 37/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4172 \n",
      "Epoch 38/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3991 \n",
      "Epoch 39/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3821 \n",
      "Epoch 40/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3656 \n",
      "Epoch 41/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3500 \n",
      "Epoch 42/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3353 \n",
      "Epoch 43/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3214 \n",
      "Epoch 44/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3083 \n",
      "Epoch 45/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2960 \n",
      "Epoch 46/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2843 \n",
      "Epoch 47/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2734 \n",
      "Epoch 48/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2630 \n",
      "Epoch 49/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2532 \n",
      "Epoch 50/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2440 \n",
      "Epoch 51/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2352 \n",
      "Epoch 52/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2270 \n",
      "Epoch 53/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2191 \n",
      "Epoch 54/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2117 \n",
      "Epoch 55/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2047 \n",
      "Epoch 56/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1980 \n",
      "Epoch 57/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1916 \n",
      "Epoch 58/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1856 \n",
      "Epoch 59/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1798 \n",
      "Epoch 60/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1744 \n",
      "Epoch 61/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1691 \n",
      "Epoch 62/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1642 \n",
      "Epoch 63/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1594 \n",
      "Epoch 64/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1549 \n",
      "Epoch 65/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1505 \n",
      "Epoch 66/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1464 \n",
      "Epoch 67/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1424 \n",
      "Epoch 68/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1386 \n",
      "Epoch 69/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1349 \n",
      "Epoch 70/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1314 \n",
      "Epoch 71/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1280 \n",
      "Epoch 72/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1248 \n",
      "Epoch 73/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1217 \n",
      "Epoch 74/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1187 \n",
      "Epoch 75/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1158 \n",
      "Epoch 76/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1130 \n",
      "Epoch 77/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1104 \n",
      "Epoch 78/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1078 \n",
      "Epoch 79/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1053 \n",
      "Epoch 80/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1029 \n",
      "Epoch 81/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1006 \n",
      "Epoch 82/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0984 \n",
      "Epoch 83/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0963 \n",
      "Epoch 84/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0942 \n",
      "Epoch 85/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0922 \n",
      "Epoch 86/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0902 \n",
      "Epoch 87/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0884 \n",
      "Epoch 88/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0866 \n",
      "Epoch 89/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0848 \n",
      "Epoch 90/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0831 \n",
      "Epoch 91/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0815 \n",
      "Epoch 92/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0799 \n",
      "Epoch 93/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0783 \n",
      "Epoch 94/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0768 \n",
      "Epoch 95/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0754 \n",
      "Epoch 96/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0740 \n",
      "Epoch 97/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0727 \n",
      "Epoch 98/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0714 \n",
      "Epoch 99/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0701 \n",
      "Epoch 100/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0689 \n",
      "Epoch 101/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0677 \n",
      "Epoch 102/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0666 \n",
      "Epoch 103/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0655 \n",
      "Epoch 104/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0644 \n",
      "Epoch 105/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0633 \n",
      "Epoch 106/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0623 \n",
      "Epoch 107/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0613 \n",
      "Epoch 108/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0603 \n",
      "Epoch 109/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0594 \n",
      "Epoch 110/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0585 \n",
      "Epoch 111/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0576 \n",
      "Epoch 112/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0567 \n",
      "Epoch 113/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0558 \n",
      "Epoch 114/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0550 \n",
      "Epoch 115/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0542 \n",
      "Epoch 116/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0534 \n",
      "Epoch 117/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0527 \n",
      "Epoch 118/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0519 \n",
      "Epoch 119/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0512 \n",
      "Epoch 120/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0505 \n",
      "Epoch 121/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0498 \n",
      "Epoch 122/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0491 \n",
      "Epoch 123/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0484 \n",
      "Epoch 124/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0478 \n",
      "Epoch 125/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0471 \n",
      "Epoch 126/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0465 \n",
      "Epoch 127/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0459 \n",
      "Epoch 128/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0453 \n",
      "Epoch 129/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0447 \n",
      "Epoch 130/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0442 \n",
      "Epoch 131/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0436\n",
      "Epoch 132/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0431 \n",
      "Epoch 133/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0425 \n",
      "Epoch 134/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0420 \n",
      "Epoch 135/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0415 \n",
      "Epoch 136/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0410 \n",
      "Epoch 137/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0405 \n",
      "Epoch 138/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0400 \n",
      "Epoch 139/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0396 \n",
      "Epoch 140/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0391 \n",
      "Epoch 141/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0386 \n",
      "Epoch 142/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0382 \n",
      "Epoch 143/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0378 \n",
      "Epoch 144/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0373 \n",
      "Epoch 145/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0369 \n",
      "Epoch 146/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0365 \n",
      "Epoch 147/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0361 \n",
      "Epoch 148/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0357 \n",
      "Epoch 149/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0353 \n",
      "Epoch 150/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0349 \n",
      "Epoch 151/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0346 \n",
      "Epoch 152/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0342 \n",
      "Epoch 153/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0338 \n",
      "Epoch 154/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0335 \n",
      "Epoch 155/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0331 \n",
      "Epoch 156/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0328 \n",
      "Epoch 157/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0324 \n",
      "Epoch 158/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0321 \n",
      "Epoch 159/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0318 \n",
      "Epoch 160/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0315 \n",
      "Epoch 161/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0312 \n",
      "Epoch 162/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0308 \n",
      "Epoch 163/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0305 \n",
      "Epoch 164/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0302 \n",
      "Epoch 165/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0299 \n",
      "Epoch 166/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0297 \n",
      "Epoch 167/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0294 \n",
      "Epoch 168/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0291 \n",
      "Epoch 169/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0288 \n",
      "Epoch 170/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0285 \n",
      "Epoch 171/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0283 \n",
      "Epoch 172/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0280 \n",
      "Epoch 173/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0277 \n",
      "Epoch 174/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0275 \n",
      "Epoch 175/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0272 \n",
      "Epoch 176/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0270 \n",
      "Epoch 177/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0267 \n",
      "Epoch 178/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0265 \n",
      "Epoch 179/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0263 \n",
      "Epoch 180/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0260 \n",
      "Epoch 181/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0258 \n",
      "Epoch 182/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0256 \n",
      "Epoch 183/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0253 \n",
      "Epoch 184/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0251 \n",
      "Epoch 185/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0249 \n",
      "Epoch 186/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0247 \n",
      "Epoch 187/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0245 \n",
      "Epoch 188/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0243 \n",
      "Epoch 189/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0241 \n",
      "Epoch 190/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0239 \n",
      "Epoch 191/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0237 \n",
      "Epoch 192/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0235 \n",
      "Epoch 193/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0233 \n",
      "Epoch 194/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0231 \n",
      "Epoch 195/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0229 \n",
      "Epoch 196/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0227 \n",
      "Epoch 197/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0225 \n",
      "Epoch 198/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0223 \n",
      "Epoch 199/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0221 \n",
      "Epoch 200/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0220 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2457d4d28b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1234) #applied to achieve consistent results\n",
    "model = Sequential([\n",
    "    Dense(2, activation='relu', name=\"L1\"),\n",
    "    Dense(4, activation='linear', name=\"L2\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.01),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9141d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering the trained parameters from the first layer\n",
    "l1 = model.get_layer(\"L1\")\n",
    "W1,b1 = l1.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e74bcc",
   "metadata": {},
   "source": [
    "### Advanced optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440369b",
   "metadata": {},
   "source": [
    "Gradient descent is a optimization algorithm that is widely used throughout ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334baf79",
   "metadata": {},
   "source": [
    "Remember that gradient descent is:\n",
    "$$w_j = w_j - \\alpha\\frac{\\partial}{\\partial w_j}J(\\vec{w},b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0da35e",
   "metadata": {},
   "source": [
    "In this case it takes small steps towards the minimum of the cost function. But what if there could be an algorithms that can adjust the learning rate to make bigger steps?  \n",
    "\n",
    "This algorithm is called the ***Adam algorithm***. This algorithm can adjust the *learning rate* in such a manner that if it is too big, it can make it smaller and if it is too small it can make it bigger. It *optimizes* the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7540516",
   "metadata": {},
   "source": [
    "Adam - *Adaptive Moment estimation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60812c67",
   "metadata": {},
   "source": [
    "This algorithm also uses a different *learning* rate with every parameter $w_1, w_2, w_3, ..., b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063402d",
   "metadata": {},
   "source": [
    "**The logic behind the algorithm is**:\n",
    "* if the parameter keeps moving in the same direction > Then increase the learning rate and,\n",
    "* if the parameter keeps oscillating > Then reduce the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071a805",
   "metadata": {},
   "source": [
    "### Alternative Layer Types (Convolutional NNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c6223",
   "metadata": {},
   "source": [
    "**All the layers explored so far were the *Dense*, this means that every neuron in a layer gets all the activations from the previous layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9c3c8",
   "metadata": {},
   "source": [
    "Using this type you can still build very powerful NNs.  \n",
    "\n",
    "However, there are NNs with other properties. Another type is called the ***Convolutional*** Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48edd12",
   "metadata": {},
   "source": [
    "How does **Convolutional layer** work (image example):\n",
    "* Rather than having each neuron can look at the entire image, we have the neuron just being able to look at a single part of the image\n",
    "* The second neuron also looks at the pixels in a limited region. These regions are random\n",
    "* Why do we do this?\n",
    "    * Speeds up the computation\n",
    "    * Needs less training data\n",
    "    * Less prone to overfitting\n",
    "\n",
    "How does the **CNN** work:\n",
    "* Given a EKG that has a time-series of say $x_1$ to $x_{100}$\n",
    "* Take the neurons in the *Convolutional* layer and each neuron will only get parts of tge time-series\n",
    "    * e.g., neuron one gets $x_1$ to $x_{20}$, neuron two gets $x_{11}$ to $x_{30}$, neuron 3 gets $x_21$ to $x_40$ and so on\n",
    "    * Say this layer has 9 units\n",
    "* The next unit (also *Convolutional*) has, say 3 units\n",
    "    * In this unit the neurons also only get to see part of the activations produced by the first layer\n",
    "    * e.g., neuron one gets $a_1$ to $a_{5}$, neuron two gets $a_{3}$ to $a_{7}$, and so on\n",
    "* Finally the output layer is a sigmoid that will take all the activations of the last hidden layer and compute the probability  \n",
    "\n",
    "Here there are quite some **architectural** choices:\n",
    "* How big is the *window* of inputs each neuron can look at?\n",
    "* How many neurons per layer?  \n",
    "\n",
    "With this you can build more effective NNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb4714",
   "metadata": {},
   "source": [
    "### Computation Graph and Backpropagation on a Large NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b455ab",
   "metadata": {},
   "source": [
    "Take a NN with with input $\\vec{x}$ and two nodes with activations $\\vec{a^{[1]}}$ and $\\vec{a^{[2]}}$ with *ReLU* activation function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003546f1",
   "metadata": {},
   "source": [
    "Given the values $w^{[1]} = 2$, $b^{[1]} = 0$, $w^{[2]} = 3$, $b^{[2]} = 1$, $x = 1$ and $y = 5$  \n",
    "And $g(z) = max(0,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca885e",
   "metadata": {},
   "source": [
    "Now we have that:  \n",
    "$\\vec{a^{[1]}} = g(w^{[1]}*\\vec{x} + b^{[1]})$ and  \n",
    "$\\vec{a^{[2]}} = g(w^{[1]}*\\vec{a^{[1]}} + b^{[2]})$, finally  \n",
    "$J(w,b) = \\frac{1}{2}(\\vec{a^{[2]}} - y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e5c39",
   "metadata": {},
   "source": [
    "The computation graph would look like (step by step calculation):  \n",
    "\n",
    "1. $t^{[1]}$ = $w^{[1]}*\\vec{x}$ = 2\n",
    "2. $z^{[1]}$ = $t^{[1]} + b^{[1]}$ = 2\n",
    "3. ${a^{[1]}} = g(z^{[1]})$ = 2\n",
    "4. $t^{[2]} = w^{[2]}*{a^{[1]}}$ = 6\n",
    "5. $z^{[2]} = t^{[2]} + b^{[2]}$ = 7\n",
    "6. ${a^{[2]}} = g(z^{[2]})$ = 7\n",
    "7. $J = \\frac{1}{2}({a^{[2]}}-y)^2 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a170a",
   "metadata": {},
   "source": [
    "Now for the **Backprop**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8085d9",
   "metadata": {},
   "source": [
    "You have to find the derivative at each point. Starting with $J$ w.r.t ${a^{[2]}}$ and working your wat back to $w^{[1]}$. To do this you will need to use the **Chain-rule** in calculus. This is a very efficient way of calculating the derivate and it can be done in N+P times for a NN of N nodes and P parameters. Instead of traditionally it would take NxP times to calculate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc271859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy \n",
    "from sympy import symbols, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c7bccab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle w^{3}$"
      ],
      "text/plain": [
       "w**3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets compare the symbolic derivative with the arithmetic calculation\n",
    "J,w = symbols('J,w')\n",
    "J = w**3\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71435d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 3 w^{2}$"
      ],
      "text/plain": [
       "3*w**2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_dw = diff(J)\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac679170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 12$"
      ],
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_dw.subs([(w,2)]) #derivative at the point of w=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2939ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8, J_epsilon = 8.012006000999998, dJ_dw~= k = 12.006000999997823\n"
     ]
    }
   ],
   "source": [
    "#Now for the arithmetic calculation:\n",
    "J = (2)**3\n",
    "J_epsilon = (2+0.001)**3\n",
    "k = (J_epsilon-J)/0.001\n",
    "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw~= k = {k}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
