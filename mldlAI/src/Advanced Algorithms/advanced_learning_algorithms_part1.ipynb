{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c43207",
   "metadata": {},
   "source": [
    "### Advanced learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446b262",
   "metadata": {},
   "source": [
    "What will be learned:\n",
    "* Neural Networks \n",
    "* Inferencing (prediction)\n",
    "* Training\n",
    "* Practical advice for ML systems\n",
    "* Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc19503",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b008eee",
   "metadata": {},
   "source": [
    "*Deeplearning* is usually the word used to refer to Neural Networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd2d35",
   "metadata": {},
   "source": [
    "Speech recognition is an example of DL. Computervision, NLP etc. are all also applications that use Neural Networks in Machine Learning. Although, it was originally inspired by how the brain works. Today, they do not really mimick the brain.\n",
    "\n",
    "**Each neuron in a NN gets an input number, does some computation on the number, and then outputs the newly computed number.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6732d",
   "metadata": {},
   "source": [
    "The ideas of NN has been around for many years, so why is it gaining this much traction now?  \n",
    "\n",
    "The amount of data digitized in this age has exploded. With traditional algorithms like linear and logistic regression cannot take advantage of this. It 'platues' eventually.  \n",
    "\n",
    "So they realized training NN takes better advantage of this data. **The size of the NN's performance is directly correlated to the amount of data that is used to train it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7350503",
   "metadata": {},
   "source": [
    "### How a NN acts and works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd255f",
   "metadata": {},
   "source": [
    "Lets use an example of selling t-shirts and you want to predict if the product will be a top seller or not:  \n",
    "* You collected data of different t-shirts and which ones became to sellers\n",
    "* input $x$ is the price of the t-shirt and you can apply logistic regression by fitting a *sigmoid* function to the data\n",
    "* $f(x)$ or now called $a$ is used to denote the output. It is also called the *activation*\n",
    "* Now you have a basic single neuron that takes input, does calculation (logistic regression) and outputs a value\n",
    "* Building a NN requires a whole bunch of these *neurons*\n",
    "\n",
    "A more complex example takes input features: price of t-shirt, shipping costs, amount of marketing of the t-shirt, and material quality:\n",
    "* Few factors that influence if the t-shirt becomes a top-seller:\n",
    "    * Affordability of the t-shirt\n",
    "    * Degree of awareness of the t-shirt\n",
    "    * Degree of quality\n",
    "* How to build the NN:\n",
    "    * Single neuron for each factor\n",
    "    * Logistic regression neuron inputs the price of t-shirt and shipping costs and outputs *affordability*\n",
    "    * Another NN to estimate awareness by inputting the *amount of marketing*\n",
    "    * Final NN to estimate the degree of quality. This will be a function of the price of the t-shirt and the material quality\n",
    "* Taking these 3 neurons (**the layer**) will be fed into a final neuron (**output layer**) that does logistic regression and then will output the probability of the t-shirt being a top seller\n",
    "* We would also refer to the outputs of the first layer (affordability, awareness and quality) as *activation*\n",
    "* Finally, the NN takes 4 values (**input layer**), calculates new values, and then finally uses these three to calculate the last number which is the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89560c4",
   "metadata": {},
   "source": [
    "In the example above we go manually through these neurons and decide which take which features. How it works in practice is that each neuron will have access to each input feature of the previous layer. The neuron will learn to ignore the irrelevant features.\n",
    "\n",
    "Each layer inputs a vector of features and outputs a vector of features. The NN has input layer, hidden layer(s), and output layer.  \n",
    "\n",
    "One way to think of a NN is it is just logistic regression. But in this version of logistic regression it can learn its own features that makes it easier to make accurate predictions. The NN can also 'engineer' its own features to make it easier for itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcfd0a",
   "metadata": {},
   "source": [
    "**NOTE**: <u>You do not need to explicitly decide which features the NN should compute, instead it figures out all by itself what are the features it wants to use in the hidden layers.<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7af803",
   "metadata": {},
   "source": [
    "When building a NN you need to make a few decisions:\n",
    "* How many hidden layers will you use\n",
    "* How many neurons will you use  \n",
    "\n",
    "These are questions of the **architecture** of the NNs. Choosing the appropriate numbers for these will have a great impact on the performance of the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0289c5",
   "metadata": {},
   "source": [
    "### Example of NN: Recognizing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25471688",
   "metadata": {},
   "source": [
    "In a face recognition example you want the NN to take the image of a person as input and output the identity. Say the picture is 1000px by 1000px. It will be represented in the computer as 1000rows by 1000 columns. Unrolling this into a single vector will be 1 million (1000x1000) values. Then the NN takes these values and tries to predict the identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35daef86",
   "metadata": {},
   "source": [
    "Input $\\vec{x}$ is fed into an input layer and then it extracts some features. The output of this first layer is fed to a second layer and so on and so on until the output layer predicts person $xyz$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3d566",
   "metadata": {},
   "source": [
    "**How it works**:  \n",
    "1) For instance the first layers will be looking at short lines or edges, \n",
    "2) as you go further it will start to look at a combination of more such lines and edges as a corner of a nose or an eye for example. \n",
    "3) Then further along it will check larger, coarser face shapes and \n",
    "4) finally the output layer that tries to determine the identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a95d1",
   "metadata": {},
   "source": [
    "**Again the NN figures out all on its own how to set itself up and figure out the identities by looking at these features in different ways.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b5e26",
   "metadata": {},
   "source": [
    "### Neural Network Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9156d",
   "metadata": {},
   "source": [
    "The first neuron has values $w_1, b_2, a_2$. After the first layer you are left with a vector of *activation* values, that can be denoted as $\\vec{a}$. This is passed to the output layer to do the final computations.  \n",
    "\n",
    "To denote the output of the first layer, it will be written as follow:  \n",
    "$$\\vec{a}^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c442979",
   "metadata": {},
   "source": [
    "This shows the activation values of the first output layer. You can also write more specific the activation value of the first neuron in the first layer:\n",
    "$$\\vec{a}^{[1]}_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1428800",
   "metadata": {},
   "source": [
    "To denote the $w$ and $b$ values will also be as follows:  \n",
    "$$\\vec{w}^{[1]}_1, \\vec{b}^{[1]}_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e929f9",
   "metadata": {},
   "source": [
    "This shows the $w$ and $b$ value for the first neuron in the first layer. The **superscript** denotes the number of the layer \"[1]\" and the subscript shows the number of the neuron in the layer, counted from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946686f",
   "metadata": {},
   "source": [
    "### Neural Network Model (More complex Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2496c8",
   "metadata": {},
   "source": [
    "The example has:\n",
    "* Four layers in total\n",
    "* One input layer (layer 0), **NOTE**: *By convention we do not count this layer when referring to the NN*\n",
    "* Three hidden layers (layer 1, 2, 3)\n",
    "* One output layer (layer 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85658bb2",
   "metadata": {},
   "source": [
    "Example of the the computations i of layer 3:  \n",
    "$$a^{[3]}_1 = g(\\vec{w}_1^{[3]}\\cdot\\vec{a}^{[2]}+b_1^{[3]})$$\n",
    "$$a^{[3]}_2 = g(\\vec{w}_2^{[3]}\\cdot\\vec{a}^{[2]}+b_2^{[3]})$$\n",
    "$$a^{[3]}_3 = g(\\vec{w}_3^{[3]}\\cdot\\vec{a}^{[2]}+b_3^{[3]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4a95b",
   "metadata": {},
   "source": [
    "Note that every Neuron takes in the whole $\\vec{a}^{[2]}$ to do the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d349d23",
   "metadata": {},
   "source": [
    "This can be written more general as:\n",
    "$$a_j^{[l]} = g(\\vec{w}_j^{[l]}\\cdot\\vec{a}^{[l-1]}+b_j^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9f5a7",
   "metadata": {},
   "source": [
    "$g$ can be referred to in general as the *activation function*. Input vector is referred to as $\\vec{x} = \\vec{a}^{[0]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695eb2cc",
   "metadata": {},
   "source": [
    "### Inference: Making Predictions (Forward Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9b145",
   "metadata": {},
   "source": [
    "In the example we will:\n",
    "* input an image and \n",
    "* try to distinguish between the digits 0 and 1. \n",
    "* The input is an 8x8 image.\n",
    "* Three layer NN: Layer 1 = 25 units, Layer 2 = 15 units, and Layer 3 = 1 unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d4e83",
   "metadata": {},
   "source": [
    "Sequence of computations: $\\vec{x}-->\\vec{a}^{[1]}-->\\vec{a}^{[2]}-->\\vec{a}^{[3]}$. With $\\vec{a}^{[3]}$ being a single scalar value that can be checked to be 1 if $\\vec{a}^{[3]}>=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72c1f8",
   "metadata": {},
   "source": [
    "Because this algorithm goes from left to right, this is also called **Forward Propagation**. This is because you are *propagating* the activations of the neurons forward. The architecture here is fairly typical for NNs with the number of neurons decreasing from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b742b9",
   "metadata": {},
   "source": [
    "### TensorFlow Implementation: Inference in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2d0a8",
   "metadata": {},
   "source": [
    "TensorFlow is one of the leading tools for building algorithms.  \n",
    "\n",
    "Example of roasting coffee:\n",
    "* *Two params* or features: Temp (celsius), and Duration (minutes)\n",
    "* Dataset will have different temperatures and durations, and the *label* showing whether the coffee roasted is good tasting or not (y = 1 is good and y = 0 is bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e5aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "x = np.array([[200.0, 17.0]])\n",
    "layer_1 = Dense(units=3, activation='sigmoid') #Dense is another name for the layers. 3 hidden units with act func sigmoid\n",
    "a1 = layer_1(x) #Apply layer_1 to x\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "a2 = layer_2(a1)\n",
    "yhat = 1 if a2 > 0.5 else 0\n",
    "#Here we did not load w and b and so on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef98fb",
   "metadata": {},
   "source": [
    "### Data in TensorFlow: How TensorFlow handles data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb27bd",
   "metadata": {},
   "source": [
    "How data is represented in numpy? Why do you have the double square brackets in 'x =np.array([[200.0, 17.0]])'? \n",
    "\n",
    "To store a 2x3 matrix: x = np.array([[1, 2, 3],[4, 5, 6]]).  \n",
    "\n",
    "**When we set x =np.array([[200.0, 17.0]]) we created a 1x2 matrix as follows: [200 17].**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d880019",
   "metadata": {},
   "source": [
    "When you want a 1x2 matrix it will look like this: x = np.array([[200],[17]]).  \n",
    "\n",
    "**So why does it have double square brackets?** *It's because one of the dimensions is 2D matrix that we want, otherwise it would be x = np.array([200, 17]) and this would be a 1D \"vector\" or a linear array*  \n",
    "\n",
    "We used this 1D vector to represent the features $x$ in course 1. But with Tensorflow there is a convention to use matrices to represent the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb9d90",
   "metadata": {},
   "source": [
    "If you print a1, you will get:  \n",
    "a1 = layer_1(x)  \n",
    "**result**: tf.Tensor([[0.2 0.7 0.3]], shape(1, 3) dtype=float32)\n",
    "\n",
    "a1.numpy()  \n",
    "**result**: array([[0.2, 0.7, 0.3]], dtype=float32)\n",
    "\n",
    "if we look at a2:\n",
    "a2 = layer_2(a1)  \n",
    "**result**: tf.Tensor([[0.8]], shape=(1, 1), dtype=float32)\n",
    "\n",
    "a2.numpy()  \n",
    "**result**: array([[0.8]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73366421",
   "metadata": {},
   "source": [
    "### Building a Neural network in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae53644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create the first two layers\n",
    "layer_1 = Dense(units=3, activation='sigmoid') #Dense is another name for the layers. 3 hidden units with act func sigmoid\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "#Then we can tell Tensorflow to take these layers and string them together in a NN\n",
    "model = Sequential([layer_1, layer_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c583b1",
   "metadata": {},
   "source": [
    "When coding in Tensorflow, we do not explicitly code the layers, it is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Dense(units=3, activation=\"sigmoid\"),\n",
    "                    Dense(units=1, activation=\"sigmoid\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80a947",
   "metadata": {},
   "source": [
    "### Implementing NN efficiently (Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb973a7",
   "metadata": {},
   "source": [
    "Vectorized Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[200, 17]]) #2D-array/Matrix\n",
    "W = np.array([[1, -3, 5],\n",
    "              [-2, 4, -6]])#2D-array/Matrix\n",
    "B = np.array([[-1, 1, 2]])#2D-array/Matrix\n",
    "\n",
    "def dense(A_in, W, B):\n",
    "    Z = np.matmul(A_in,W) + B #Matmul does the full matrix multiplication\n",
    "    A_out = g(Z)\n",
    "    return A_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21539bb1",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97882b",
   "metadata": {},
   "source": [
    "Taking the dot product, you multiply the first elements with each other, then the second, the third and then add them up. e.g.  \n",
    "\n",
    "$\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = (1*3)+(2*4) = 3+8 = 11$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9465533",
   "metadata": {},
   "source": [
    "Transpose:  Takes a column vector and transforms it to a row vector\n",
    "\n",
    "$\\vec{a} = \\begin{bmatrix} 1\\\\2 \\end{bmatrix} = \\vec{a}^T = \\begin{bmatrix} 1&&2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f002867",
   "metadata": {},
   "source": [
    "e.g. multiplying $\\vec{a}^T =\\begin{bmatrix} 1&&2 \\end{bmatrix}   \\vec{w} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$  \n",
    "\n",
    "***Is the same as taking the dot product.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1ed05",
   "metadata": {},
   "source": [
    "**Vector Matrix multiplication**:  \n",
    "\n",
    "$\\vec{a}^T = \\begin{bmatrix} 1&&2 \\end{bmatrix}$  \n",
    "\n",
    "$W = \\begin{bmatrix} 3&&5 \\\\ 4&&6 \\end{bmatrix}$  \n",
    "\n",
    "Then calculating: $Z = \\vec{a}^T W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a987cae",
   "metadata": {},
   "source": [
    "$Z = \\begin{bmatrix}\\vec{a}^T \\vec{w_1} && \\vec{a}^T \\vec{w_2}\\end{bmatrix}$ which is $Z = [(1*3)+(2*4)\\; (1*5)+(2*6)] = [11 \\; 17]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6178703",
   "metadata": {},
   "source": [
    "**Matrix Matrix Multiplication**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feebef0",
   "metadata": {},
   "source": [
    "$A = \\begin{bmatrix}1&&-1 \\\\ 2&&-2\\end{bmatrix}$  \n",
    "\n",
    "$A^T = \\begin{bmatrix}1&&2 \\\\ -1&&-2\\end{bmatrix}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d3440",
   "metadata": {},
   "source": [
    "**How to make the transpose**? The first column, becomes the first row, and the second column becomes the second row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b8187",
   "metadata": {},
   "source": [
    "We also have: $W = \\begin{bmatrix} 3&&5 \\\\ 4&&6 \\end{bmatrix}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4b442",
   "metadata": {},
   "source": [
    "$Z = A^TW = \\begin{bmatrix} Row(a1)Col(w1)&&Row(a1)Col(w2) \\\\ Row(a2)Col(w1)&&Row(a2)Col(w2) \\end{bmatrix}$  \n",
    "\n",
    "$Z = \\begin{bmatrix} 11&&17 \\\\ -11&&-17 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce624e7",
   "metadata": {},
   "source": [
    "### Matrix Multiplication Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d9916e",
   "metadata": {},
   "source": [
    "Think of the columns of each matrix as a vector.  \n",
    "\n",
    "If you take the $Z = A^TW$.  \n",
    "\n",
    "Think of it as each row of $A^T$ corresponds to the row of the result and each column of $W$ corresponds to each column of the result.  \n",
    "\n",
    "**A requirement**: 3x2 matrix can only be multiplied with 2xN matrix. The result will be a 3xN matrix.  \n",
    "\n",
    "**Why?** because you can only take dot products of vectors that are the same length! Therefore the columns of $A^T$ must be the same length as the rows of $W$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
