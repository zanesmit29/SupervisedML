{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94321ae1",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43944758",
   "metadata": {},
   "source": [
    "Output variable $y$ can only take on a limited range of outputs.  \n",
    "\n",
    "Classic classification problems:\n",
    "* Spam filter\n",
    "* Fraud detection in transactions (fintech)\n",
    "* Tumore classification (malignant or not)  \n",
    "\n",
    "**Binary Classification** refers to there being only two *classes* or *binaries*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc003b",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5b97a",
   "metadata": {},
   "source": [
    "One of the most widely used ML algorithms\n",
    "\n",
    "The **Sigmoid function** or **Logistic function**:\n",
    "* Function in the shape of an 'S'\n",
    "* Always outputs values between 0 and 1\n",
    "* Can be written as $\\frac{1}{1 + e^{-z}}$ where $ 0<g(z)<1$\n",
    "* When $z$ is large $g(z)$ becomes very close to 1 and when $z$ is a large negative number $g(z)$ becomes very close to 0\n",
    "* When $z$ is exactly 1 then $g(z) = 0.5$  \n",
    "\n",
    "\n",
    "$z = \\vec{w}\\cdot\\vec{x} + b$ which is the same as $f_{\\vec{w},b}(\\vec{x})$  \n",
    "\n",
    "Then plug this into $\\frac{1}{1 + e^{-z}}$, this equivalently gives $f_{\\vec{w},b}(\\vec{x}) = g(\\vec{w}\\cdot\\vec{x} + b)$, therefore:  \n",
    "\n",
    "$$\\frac{1}{1 + e^{-(\\vec{w}\\cdot\\vec{x} + b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ccd59",
   "metadata": {},
   "source": [
    "If you run the model for tumor classification and you get $f_{\\vec{w},b}(\\vec{x}) = 0.7$ that means the model is predicting there is a 70% chance that y = 1. That also means that there is a 30% chance that y = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fca2feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input array:, [1 2 3]\n",
      "Exponent array:, [ 2.718  7.389 20.086]\n"
     ]
    }
   ],
   "source": [
    "# Example code of the sigmoid function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_array = np.array([1,2,3])\n",
    "exp_array = np.exp(input_array)\n",
    "print(f\"Input array:, {input_array}\")\n",
    "print(f\"Exponent array:, {exp_array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ea193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Compute the sigmoid of z\n",
    "    # Args: z (ndarray): A scalar, numpy array of any size\n",
    "    # Returns g (ndarray): sigmoid(z), with the same shape as z\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec12226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (z), Output (sigmoid(z))\n",
      "[[-1.000e+01  4.540e-05]\n",
      " [-9.000e+00  1.234e-04]\n",
      " [-8.000e+00  3.354e-04]\n",
      " [-7.000e+00  9.111e-04]\n",
      " [-6.000e+00  2.473e-03]\n",
      " [-5.000e+00  6.693e-03]\n",
      " [-4.000e+00  1.799e-02]\n",
      " [-3.000e+00  4.743e-02]\n",
      " [-2.000e+00  1.192e-01]\n",
      " [-1.000e+00  2.689e-01]\n",
      " [ 0.000e+00  5.000e-01]\n",
      " [ 1.000e+00  7.311e-01]\n",
      " [ 2.000e+00  8.808e-01]\n",
      " [ 3.000e+00  9.526e-01]\n",
      " [ 4.000e+00  9.820e-01]\n",
      " [ 5.000e+00  9.933e-01]\n",
      " [ 6.000e+00  9.975e-01]\n",
      " [ 7.000e+00  9.991e-01]\n",
      " [ 8.000e+00  9.997e-01]\n",
      " [ 9.000e+00  9.999e-01]\n",
      " [ 1.000e+01  1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Generate an evenly spaced array of values between -10 and 10\n",
    "\n",
    "z_tmp = np.arange(-10,11)\n",
    "y = sigmoid(z_tmp)\n",
    "\n",
    "# Pretty printing two arrays\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"Input (z), Output (sigmoid(z))\")\n",
    "print(np.c_[z_tmp, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a3a045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'z')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLotting with Matplotlib\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,3))\n",
    "ax.plot(z_tmp,y,c=\"b\")\n",
    "\n",
    "ax.set_title(\"Sigmoid function\")\n",
    "ax.set_ylabel(\"sigmoid(z)\")\n",
    "ax.set_xlabel(\"z\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68636e36",
   "metadata": {},
   "source": [
    "### Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875201",
   "metadata": {},
   "source": [
    "What if you want to predict of y = 1 or 0.  \n",
    "\n",
    "Then you have to pick a *threshold* e.g. less than 0.5 is 0 and more than 0.5 is 1.  \n",
    "\n",
    "Where is $f_{\\vec{w},b}(\\vec{x}) >= 0.5$?, when $g(z) >= 0.5, when $z>=0$ and therefore, $\\vec{w}\\cdot\\vec{x} + b >= 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce6d3f",
   "metadata": {},
   "source": [
    "For an example that has two features $x_1$ and $x_2$ we have that $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_1x_1 + w_2x_2 + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8872d3e",
   "metadata": {},
   "source": [
    "**The decision boundary is what you want to look at**. This is where $z = \\vec{w}\\cdot\\vec{x} + b = 0$  \n",
    "\n",
    "For *non-linear* decision boundaries:  \n",
    "\n",
    "$z = w_1x_1^2 + w_2x_2^2 + b$, if we let $w_1$ and $w_2$ equal to 1 and $b$ equal to -1, then the **decision boundary** will be written as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd3a13",
   "metadata": {},
   "source": [
    "\n",
    "$$z = x_1^2 + x_2^2 - 1 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426f2a2",
   "metadata": {},
   "source": [
    "therefore  \n",
    "$$x_1^2 + x_2^2 = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679450c1",
   "metadata": {},
   "source": [
    "is the function of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24adf9c1",
   "metadata": {},
   "source": [
    "### Loss & Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3269e",
   "metadata": {},
   "source": [
    "***Reminder*: The cost function gives you a way to measure how well a specific set of parameters fit the training data, thereby gives you a way to choose better parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb852ec",
   "metadata": {},
   "source": [
    "A example has $i$ training examples $j$ number of features:\n",
    "\n",
    "Plotting the cost function $$J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m}\\frac{1}{2}(f_{\\vec{w},b}(x^i)-y^i)$$  \n",
    "\n",
    "where $$f_{\\vec{w},b}(\\vec{x}) = \\frac{1}{1 + e^{-(\\vec{w}\\cdot\\vec{x} + b)}}$$  \n",
    "\n",
    "will give you a **non-convex** function. There can actually be lots of local minima (too much!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf2778",
   "metadata": {},
   "source": [
    "**For logistic regression we wil have**:  \n",
    "\n",
    "We can say the *Loss function* can be donated as $\\frac{1}{2}(f_{\\vec{w},b}(x^i)-y^i)$, and can be said that it is equal to half of the squared difference.  \n",
    "\n",
    "The loss function can be set as:  \n",
    "\n",
    "$-log(f_{\\vec{w},b}(\\vec{x}^i))$ if $y^i = 1$, and $-log(1-f_{\\vec{w},b}(\\vec{x}^i))$ if $y^i = 0$  \n",
    "\n",
    "The loss will be the **lowest** when $f_{\\vec{w},b}(x^i)$ predicts close to the true label $y^i$   \n",
    "\n",
    "The further prediction $f_{\\vec{w},b}(x^i)$ is from the target $y^i$, the **higher** the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10b71d",
   "metadata": {},
   "source": [
    "*Loss* is a measure of difference of a single example to its target value, while *cost* is the measure of losses over the training set.  \n",
    "\n",
    "The entire loss function can be written as:  \n",
    "\n",
    "$$f_{\\vec{w},b}(\\vec{x}^i,y^i) = -(y^i)log(f_{\\vec{w},b}(\\vec{x}^i)) - (1-y^i)log(1-f_{\\vec{w},b}(\\vec{x}^i))$$\n",
    " \n",
    "This works because depending on the value of $y$ (either 1 or 0), this will cancel out the additional term and you will be left with the original loss function we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcada64",
   "metadata": {},
   "source": [
    "**Now, writing out ths cost function:**  \n",
    "\n",
    "$$J(\\vec{w},b) = -\\frac{1}{m}\\sum_{i=1}^{m}[(y^i)log(f_{\\vec{w},b}(\\vec{x}^i)) + (1-y^i)log(1-f_{\\vec{w},b}(\\vec{x}^i))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425fbe6",
   "metadata": {},
   "source": [
    "Why do we choose this specific function? It is derived from an idea in statistics called *\"maximum likelyhood estimation\"*, which is an idea in statistics how to efficiently find parameters for models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ad1fe",
   "metadata": {},
   "source": [
    "### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9ccefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b18b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X_train = np.array([[0.5, 1.5],[1, 1],[1.5, 0.5],[3, 0.5],[2, 2],[1, 2.5]]) #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb46dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42dfad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_cost(X, y, w, b):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        zi = np.dot(X[i],w)+b\n",
    "        fxy = sigmoid(zi)\n",
    "        cost += -y[i]*np.log(fxy)+(1-y[i])*np.log(1-fxy)\n",
    "        \n",
    "    cost = cost/(-m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab8ad9",
   "metadata": {},
   "source": [
    "### Gradient descent implementation for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bba89",
   "metadata": {},
   "source": [
    "**Reminder**: Gradient descent is used to minimize the cost function.  \n",
    "\n",
    "Similarly to gradient descent for linear regression. Carry out the updates simultaneously. The equations for gradient descent is the same, the only change is $f_{(\\vec{w},b)}(\\vec{x}^i)$ is different. As for logistic regression this is the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fb288",
   "metadata": {},
   "source": [
    "Gradient Descent:  \n",
    "\n",
    "$$w_j = w_j - \\alpha \\frac{\\partial }{\\partial w_j}J(\\vec{w},b)$$\n",
    "and $$b = b - \\alpha \\frac{\\partial }{\\partial b}J(\\vec{w},b)$$\n",
    "where  \n",
    "\n",
    "$$\\frac{\\partial }{\\partial w_j}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^i)-y^i)x_j^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722cfb2",
   "metadata": {},
   "source": [
    "and  \n",
    "$$\\frac{\\partial }{\\partial wb}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^i)-y^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66aa0a2",
   "metadata": {},
   "source": [
    "Remember "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7d522",
   "metadata": {},
   "source": [
    "$$f_{(\\vec{w},b)}(\\vec{x}^i) = \\frac{1}{1 + e^{-(\\vec{w}\\cdot\\vec{x} + b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0befa",
   "metadata": {},
   "source": [
    "### Logistic Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53d13b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set: [0.008 0.019 0.03  0.91  1.169 0.865]\n",
      "Accuracy on training set: 0.962406015037594\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X,y)\n",
    "\n",
    "y_pred = lr_model.predict(X)\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "print(\"Accuracy on training set:\", lr_model.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9fb6c0",
   "metadata": {},
   "source": [
    "### The problem of **Overfitting** and **Underfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9357094",
   "metadata": {},
   "source": [
    "Firstly, **underfitting** is when an algorithm does not fit the data very well. Another term used to describe this is to say that the algorithm has **high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce173f4",
   "metadata": {},
   "source": [
    "**Generalization** is when you want your algorithm to do well on data that is not even in the training set. You want the algorithm to *generalize* well even on data it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76939748",
   "metadata": {},
   "source": [
    "**Overfitting** is when the algorithm starts to follow the data exactly. When looking at a plot, you can see the plotted algorithm goes exactly through all the training data points. In such a case the model will not *generalize* very well. Another term used to describe this is to say that the algorithm has **high variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3824d",
   "metadata": {},
   "source": [
    "**NOTE**: <u>Overfitting happens frequently with using higher order polynomials to try and fit to your training data perfectly<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95dddd",
   "metadata": {},
   "source": [
    "The term **high variance** comes from the fact that this algorithm could change completely even with small changes in the training set. Therefore, for example if two engineers had two slightly altered versions of the same training set, they would end up with completely different algorithms and therefore completely different predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a5748",
   "metadata": {},
   "source": [
    "### Addressing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab33d8",
   "metadata": {},
   "source": [
    "Ways to address overfitting:\n",
    "* Collecting more training examples - With the larger training set, the algorithm is likely to overfit less\n",
    "* See if you can use fewer features $(x_i)$ - If you have many features but little training data it is likely to overfit\n",
    "    * Choose the most appropriate features (feature selection)\n",
    "    * This can be disadvantageous as you may be throwing away information\n",
    "* Regularization: Reduce the impact of the features by reducing the size of parameters $w_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a9c79",
   "metadata": {},
   "source": [
    "### Cost function with *Regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e18ac",
   "metadata": {},
   "source": [
    "Let's say you have a higher order polynomial (4th order) and you want to make $w_3$ and $w_4$ small, then you would add:\n",
    "$$(Cost function) + 1000w_3 + 1000w_4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc74d8",
   "metadata": {},
   "source": [
    "This then penalizes the cost function and then it will have to find small values for $w_3$ and $w_4$ to try to minimize the *Cost function*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a3612",
   "metadata": {},
   "source": [
    "Therefore, you end up with $w_3$ and $w_4$ being very close to zero. Nearly cancelling them out and \"getting rid\" of these terms. You would then end up with a function that is closer to a quadratic function.  \n",
    "\n",
    "In real life however, you have $w_j$ features. Therefore, *Regularization* is generally implemented to penalize all features or more precisely all the $w_j$ parameters. This usually results in a smoother model that is not prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6332f9",
   "metadata": {},
   "source": [
    "Then you would do the following:  \n",
    "$$Cost function + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79fd4c",
   "metadata": {},
   "source": [
    "**NOTE**: We divide by $2m$ such that when your training set scales, the same value of $\\lambda$ should work!  \n",
    "\n",
    "Where $\\lambda$ is the **Regularization** parameter. Similar to picking a learning rate, you would also have to choose this parameter. We also do not penalize the value $b$ as it makes little difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878549c",
   "metadata": {},
   "source": [
    "What different values of lambda do:\n",
    "* For a small lambda, say 0, **it will overfit** your data as the regularization term is 0.\n",
    "* For s big lambda, you will put a big weight on the regularization term. This makes almost all values $(w_j)$ close to zero. And you will be left with $f(x)=b$ and the learning algorithms fits a straight line. **It will underfit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d2909",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461614e",
   "metadata": {},
   "source": [
    "The gradient descent expression of $w_j$ changes to:  \n",
    "$$\\frac{\\partial }{\\partial w_j}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^i)-y^i)x_j^i + \\frac{\\lambda}{m}w_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85794ae",
   "metadata": {},
   "source": [
    "It gets the last term added. For $\\frac{\\partial }{\\partial wb}J(w,b)$ it stays the same. Remember we do not regularize $b$, therefore it stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b717d0",
   "metadata": {},
   "source": [
    "When you multiply the terms out you get the following:  \n",
    "$$w_j = w_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{\\partial }{\\partial w_j}J(\\vec{w},b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbe9af",
   "metadata": {},
   "source": [
    "As you can see the last part is just the *regular term*. For the first term:  \n",
    "\n",
    "We have $\\alpha$ being a very small number, say 0.01 and $\\lambda$ also being a relatively small number, say 1, and m is say, 50. Then you have the first term being an **very small positive number**:  \n",
    "$$w_j(1-\\alpha\\frac{\\lambda}{m}) = w_j(1-0.0002) = w_j(0.9998)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6ef6d",
   "metadata": {},
   "source": [
    "Therefore the effect of this term is that for every iteration of gradient descent you take $w_j$ and multiply it with some number slightly smaller than 1. Therefore Regularization then shrinks the value of $w_j$ on each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffc978",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a24d0",
   "metadata": {},
   "source": [
    "Similar to Linear regression, you have the *Cost function* being:\n",
    "$$Cost function +  \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4a12c",
   "metadata": {},
   "source": [
    "Then *Gradient descent* will look like the following:  \n",
    "$$w_j = w_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{\\partial }{\\partial w_j}J(\\vec{w},b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612be525",
   "metadata": {},
   "source": [
    "Which is exactly the same as for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633518f",
   "metadata": {},
   "source": [
    "### Implementing Regularization in the Cost and Gradient functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e11c49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X,y,w,b,lambda_=1):\n",
    "    m = X.shape[0]\n",
    "    n = len(w)\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X,w) + b\n",
    "        cost = cost + (f_wb_i - y[i])**2\n",
    "    cost = cost/(2*m)\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost = (w[j])**2\n",
    "    reg_cost = (lambda_/(2*m))*reg_cost\n",
    "\n",
    "    total_cost = cost + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c22fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X,y,w,b,lambda_=1):\n",
    "\n",
    "\n",
    "    m = X.shape[0]\n",
    "    n = len(w)\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w)+b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost += -y[i]*np.log(f_wb_i)-(1-y[i])*np.log(f_wb_i)\n",
    "    cost = cost/(m)\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost = (w[j])**2\n",
    "    reg_cost = (lambda_/(2*m))*reg_cost\n",
    "\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8056783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X,y,w,b,lambda_):\n",
    "    \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i],w)+b) -y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*X[i,j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m)*w[j]\n",
    "    \n",
    "    return dj_db, dj_dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "251b8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X,y,w,b,lambda_):\n",
    "    \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w)+b)\n",
    "        err = f_wb_i-y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j]+ err*X[i,j]\n",
    "        dj_db = dj_db+ err\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j]+(lambda_/m)*w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7a5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
